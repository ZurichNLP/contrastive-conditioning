{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a0de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import itertools\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from tasks.pattern_matching import WinomtPatternMatchingTask\n",
    "from tasks.pattern_matching.winomt_utils.language_predictors.util import WB_GENDER_TYPES, GENDER\n",
    "from tasks.contrastive_conditioning import WinomtContrastiveConditioningTask\n",
    "from translation_models.fairseq_models import load_sota_evaluator\n",
    "from translation_models.testing_models import DictTranslationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b225df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_path = Path(\".\") / \"data\"\n",
    "winomt_enru_translations_path = data_path / \"google.ru.full.txt\"\n",
    "winomt_enru_annotator1_path = data_path / \"en-ru.annotator1.jsonl\"\n",
    "winomt_enru_annotator2_path = data_path / \"en-ru.annotator2.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d9cf3e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "with jsonlines.open(winomt_enru_annotator1_path) as f:\n",
    "  annotations1 = {line[\"Sample ID\"]: line for line in f}\n",
    "with jsonlines.open(winomt_enru_annotator2_path) as f:\n",
    "  annotations2 = {line[\"Sample ID\"]: line for line in f}\n",
    "\n",
    "# Flatten labels\n",
    "for key in annotations1:\n",
    "    annotations1[key][\"label\"] = annotations1[key][\"label\"][0]\n",
    "for key in annotations2:\n",
    "    annotations2[key][\"label\"] = annotations2[key][\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb27287",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove samples that were only partially annotated\n",
    "for key in list(annotations1.keys()):\n",
    "    if key not in annotations2:\n",
    "        del annotations1[key]\n",
    "for key in list(annotations2.keys()):\n",
    "    if key not in annotations1:\n",
    "        del annotations2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cf5a3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13114217077964607\n"
     ]
    }
   ],
   "source": [
    "# Inter-annotator agreement before data cleaning\n",
    "keys = list(annotations1.keys())\n",
    "labels1 = [annotations1[key][\"label\"] for key in keys]\n",
    "labels2 = [annotations2[key][\"label\"] for key in keys]\n",
    "kappa = cohen_kappa_score(labels1, labels2)\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614487d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "for annotations in [annotations1, annotations2]:\n",
    "    for key in keys:\n",
    "        # Treat neutral as correct\n",
    "        if annotations[key][\"label\"] == \"Both / Neutral / Ambiguous\":\n",
    "            annotations[key][\"label\"] = annotations[key][\"Gold Gender\"].title()\n",
    "        # Treat bad as wrong\n",
    "        if annotations[key][\"label\"] == \"Translation too bad to tell\":\n",
    "            annotations[key][\"label\"] = \"Male\" if annotations[key][\"Gold Gender\"] == \"female\" else \"Female\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8b4f58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2018722773194922\n"
     ]
    }
   ],
   "source": [
    "# Inter-annotator agreement after data cleaning\n",
    "keys = list(annotations1.keys())\n",
    "labels1 = [annotations1[key][\"label\"] for key in keys]\n",
    "labels2 = [annotations2[key][\"label\"] for key in keys]\n",
    "kappa = cohen_kappa_score(labels1, labels2)\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56871514",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge annotations\n",
    "annotations = list(itertools.chain(annotations1.values(), annotations2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d8d6da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load translations\n",
    "with open(winomt_enru_translations_path) as f:\n",
    "    translations = {line.split(\" ||| \")[0].strip(): line.split(\" ||| \")[1].strip() for line in f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef31a44b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3888it [00:00, 823608.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run classic (pattern-matching) WinoMT\n",
    "winomt_pattern_matching = WinomtPatternMatchingTask(\n",
    "    tgt_language=\"ru\",\n",
    "    skip_neutral_gold=False,\n",
    "    verbose=True,\n",
    ")\n",
    "pattern_matching_evaluated_samples = winomt_pattern_matching.evaluate(DictTranslationModel(translations)).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf9a075",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/user/vamvas/.cache/torch/hub/pytorch_fairseq_master\n",
      "Loading codes from /home/user/vamvas/.cache/torch/pytorch_fairseq/15bca559d0277eb5c17149cc7e808459c6e307e5dfbb296d0cf1cfe89bb665d7.ded47c1b3054e7b2d78c0b86297f36a170b7d2e7980d8c29003634eb58d973d9/bpecodes ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading codes from /home/user/vamvas/.cache/torch/hub/en24k.fastbpe.code ...\n",
      "Read 24000 codes from the codes file.\n",
      "Loading codes from /home/user/vamvas/.cache/torch/hub/ru24k.fastbpe.code ...\n",
      "Read 24000 codes from the codes file.\n",
      "100%|████████████████████████████████████████████████████████████| 3888/3888 [02:31<00:00, 25.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 3888/3888 [02:17<00:00, 28.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run contrastive conditioning\n",
    "evaluator_model = load_sota_evaluator(\"ru\")\n",
    "winomt_contrastive_conditioning = WinomtContrastiveConditioningTask(\n",
    "    evaluator_model=evaluator_model,\n",
    "    skip_neutral_gold=False,\n",
    "    category_wise_weighting=True,\n",
    ")\n",
    "contrastive_conditioning_weighted_evaluated_samples = winomt_contrastive_conditioning.evaluate(DictTranslationModel(translations)).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09546b32",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create unweighted contrastive conditioning samples\n",
    "contrastive_conditioning_unweighted_evaluated_samples = deepcopy(contrastive_conditioning_weighted_evaluated_samples)\n",
    "for sample in contrastive_conditioning_unweighted_evaluated_samples:\n",
    "    sample.weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb65075",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        male      0.826     0.844     0.835     321.0\n",
      "      female      0.537     0.504     0.520     115.0\n",
      "\n",
      "    accuracy                          0.755     436.0\n",
      "   macro avg      0.682     0.674     0.678     436.0\n",
      "weighted avg      0.750     0.755     0.752     436.0\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        male      0.769     0.900     0.829     321.0\n",
      "      female      0.467     0.243     0.320     115.0\n",
      "\n",
      "    accuracy                          0.727     436.0\n",
      "   macro avg      0.618     0.572     0.575     436.0\n",
      "weighted avg      0.689     0.727     0.695     436.0\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        male      0.801     0.960     0.873  297705.0\n",
      "      female      0.522     0.155     0.239   84127.0\n",
      "\n",
      "    accuracy                          0.782  381832.0\n",
      "   macro avg      0.661     0.558     0.556  381832.0\n",
      "weighted avg      0.739     0.782     0.733  381832.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "for evaluated_samples in [\n",
    "    pattern_matching_evaluated_samples,\n",
    "    contrastive_conditioning_unweighted_evaluated_samples,\n",
    "    contrastive_conditioning_weighted_evaluated_samples,\n",
    "]:\n",
    "    predicted_labels = []\n",
    "    gold_labels = []\n",
    "    weights = []\n",
    "    for annotation in annotations:\n",
    "        gold_labels.append(WB_GENDER_TYPES[annotation[\"label\"].lower()].value)\n",
    "        sample_index = int(annotation[\"Index\"])\n",
    "        evaluated_sample = evaluated_samples[sample_index]\n",
    "        assert evaluated_sample.sentence == annotation[\"Source Sentence\"]\n",
    "        if hasattr(evaluated_sample, \"predicted_gender\"):\n",
    "            predicted_gender = evaluated_sample.predicted_gender.value\n",
    "            # Convert neutral or unknown to gold in order to treat classic WinoMT as fairly as possible\n",
    "            if predicted_gender in {GENDER.neutral.value, GENDER.unknown.value}:\n",
    "                predicted_gender = evaluated_sample.gold_gender.value\n",
    "        else:\n",
    "            if evaluated_sample.is_correct:\n",
    "                predicted_gender = WB_GENDER_TYPES[evaluated_sample.gold_gender].value\n",
    "            else:\n",
    "                predicted_gender = int(not WB_GENDER_TYPES[evaluated_sample.gold_gender].value)\n",
    "        predicted_labels.append(predicted_gender)\n",
    "        weights.append(getattr(evaluated_sample, \"weight\", 1))\n",
    "    class_labels = [gender.value for gender in GENDER][:2]\n",
    "    target_names = [gender.name for gender in GENDER][:2]\n",
    "    print(classification_report(\n",
    "        y_true=gold_labels,\n",
    "        y_pred=predicted_labels,\n",
    "        labels=class_labels,\n",
    "        target_names=target_names,\n",
    "        sample_weight=weights,\n",
    "        zero_division=True,\n",
    "        digits=3,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55f974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
