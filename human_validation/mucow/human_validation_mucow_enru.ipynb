{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a0de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import itertools\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from tasks.contrastive_conditioning import MucowContrastiveConditioningTask\n",
    "from translation_models.fairseq_models import load_sota_evaluator\n",
    "from translation_models.testing_models import DictTranslationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416dc45e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "mucow_path = Path(\".\").parent.parent.parent / \"data\" / \"mucow\"\n",
    "data_path = Path(\".\") / \"data\"\n",
    "pattern_matching_log_path = data_path / \"mucow_pattern_matching.results.en-ru.ensemble.log\"\n",
    "mucow_enru_annotator1_path = data_path / \"en-ru.annotator1.jsonl\"\n",
    "mucow_enru_annotator2_path = data_path / \"en-ru.annotator2.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7624b42d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "with jsonlines.open(mucow_enru_annotator1_path) as f:\n",
    "  annotations1 = {line[\"Sample ID\"]: line for line in f}\n",
    "with jsonlines.open(mucow_enru_annotator2_path) as f:\n",
    "  annotations2 = {line[\"Sample ID\"]: line for line in f}\n",
    "\n",
    "# Flatten labels\n",
    "for key in annotations1:\n",
    "    annotations1[key][\"label\"] = annotations1[key][\"label\"][0]\n",
    "for key in annotations2:\n",
    "    annotations2[key][\"label\"] = annotations2[key][\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36029a40",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 90 + 90\n"
     ]
    }
   ],
   "source": [
    "# Remove samples that were only partially annotated\n",
    "for key in list(annotations1.keys()):\n",
    "    if key not in annotations2:\n",
    "        del annotations1[key]\n",
    "for key in list(annotations2.keys()):\n",
    "    if key not in annotations1:\n",
    "        del annotations2[key]\n",
    "print(f\"Number of annotations: {len(annotations1)} + {len(annotations2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344c62d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5696465696465697\n"
     ]
    }
   ],
   "source": [
    "# Inter-annotator agreement before data cleaning\n",
    "keys = list(annotations1.keys())\n",
    "labels1 = [annotations1[key][\"label\"] for key in keys]\n",
    "labels2 = [annotations2[key][\"label\"] for key in keys]\n",
    "kappa = cohen_kappa_score(labels1, labels2)\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f41e286a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 86 + 86\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "skipped_keys = set()\n",
    "for annotations in [annotations1, annotations2]:\n",
    "    for key in keys:\n",
    "        # Treat neutral as correct\n",
    "        if annotations[key][\"label\"] == \"Both / Neutral / Ambiguous\":\n",
    "            annotations[key][\"label\"] = \"Correct Sense\"\n",
    "        # Treat bad translations as wrong\n",
    "        if annotations[key][\"label\"] == \"Translation too bad to tell / Third sense\":\n",
    "            annotations[key][\"label\"] = \"Wrong Sense\"\n",
    "        # Skip bad samples\n",
    "        if annotations[key][\"label\"] == \"Bad sample / Ill-defined senses\":\n",
    "            skipped_keys.add(key)\n",
    "for annotations in [annotations1, annotations2]:\n",
    "    for key in skipped_keys:\n",
    "        if key in annotations:\n",
    "            del annotations[key]\n",
    "print(f\"Number of annotations: {len(annotations1)} + {len(annotations2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0fbebb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8767908309455588\n"
     ]
    }
   ],
   "source": [
    "# Inter-annotator agreement after data cleaning\n",
    "keys = list(annotations1.keys())\n",
    "labels1 = [annotations1[key][\"label\"] for key in keys]\n",
    "labels2 = [annotations2[key][\"label\"] for key in keys]\n",
    "kappa = cohen_kappa_score(labels1, labels2)\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e447ad46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge annotations\n",
    "annotations = list(itertools.chain(annotations1.values(), annotations2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2341ff5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load task\n",
    "mucow_enru = MucowContrastiveConditioningTask(\n",
    "    tgt_language=\"ru\",\n",
    "    evaluator_model=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d7a03b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full number of uncovered samples:  170\n"
     ]
    }
   ],
   "source": [
    "# Create list of annotated (= uncovered) samples\n",
    "uncovered_samples = []\n",
    "all_samples_dict = {(sample.src_sentence, sample.src_word): sample for sample in mucow_enru.samples}\n",
    "for annotation in annotations:\n",
    "    try:\n",
    "        sample = all_samples_dict[(annotation[\"Source Sentence\"], annotation[\"Word\"])]\n",
    "    except KeyError:  # Google Sheets removes leading apostrophe\n",
    "        sample = all_samples_dict.get((\"'\" + annotation[\"Source Sentence\"], annotation[\"Word\"]), None)\n",
    "    if sample is None:\n",
    "        continue\n",
    "    sample._gold_label = annotation[\"label\"] == \"Correct Sense\"\n",
    "    sample.translation = annotation[\"Translation\"]\n",
    "    uncovered_samples.append(sample)\n",
    "print(\"Full number of uncovered samples: \", len(uncovered_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ebd751",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of covered samples 0.8155136268343816\n",
      "Full number of covered samples:  389\n"
     ]
    }
   ],
   "source": [
    "# Create list of unannotated (= covered) samples based on log file\n",
    "num_samples = 0\n",
    "covered_samples = []\n",
    "all_samples_dict = {(sample.src_sentence, sample.src_word): sample for sample in mucow_enru.samples}\n",
    "with jsonlines.open(pattern_matching_log_path) as f:\n",
    "    for line in f:\n",
    "        sample = all_samples_dict.get((line[\"sentence\"], line[\"src_word\"]), None)\n",
    "        if sample is None:\n",
    "            continue  # contrastive conditioning not applicable\n",
    "        if line[\"corpus\"] == \"opensubs\":  # Only evaluate on in-domain samples because they have higher quality\n",
    "            continue\n",
    "        num_samples += 1\n",
    "        if line[\"is_unknown\"]:  # = uncovered\n",
    "            continue\n",
    "        sample._gold_label = line[\"is_correct\"]\n",
    "        sample.translation = line[\"translation\"]\n",
    "        covered_samples.append(sample)\n",
    "random.seed(42)\n",
    "coverage = len(covered_samples) / num_samples\n",
    "print(\"Proportion of covered samples\", coverage)\n",
    "print(\"Full number of covered samples: \", len(covered_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "328b1057",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 921 covered samples and 170 uncovered samples\n"
     ]
    }
   ],
   "source": [
    "# Sample a proportionate amount of covered samples\n",
    "_covered_samples = []\n",
    "for _ in range(int(len(uncovered_samples) * (1 / (1 - coverage)))):\n",
    "    _covered_samples.append(random.choice(covered_samples))\n",
    "covered_samples = _covered_samples\n",
    "print(f\"Testing on {len(covered_samples)} covered samples and {len(uncovered_samples)} uncovered samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4740a2d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of agreement:  0.8973418881759854\n"
     ]
    }
   ],
   "source": [
    "# Evaluate classic MuCoW\n",
    "# Count all covered samples as agreements; judge all unknown samples as incorrect translations\n",
    "num_agreements = len(covered_samples) + sum(1 for sample in uncovered_samples if not sample._gold_label)\n",
    "proportion_of_agreement = num_agreements / (len(covered_samples) + len(uncovered_samples))\n",
    "print(\"Proportion of agreement: \", proportion_of_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b4c7869",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/user/vamvas/.cache/torch/hub/pytorch_fairseq_master\n",
      "Loading codes from /home/user/vamvas/.cache/torch/pytorch_fairseq/15bca559d0277eb5c17149cc7e808459c6e307e5dfbb296d0cf1cfe89bb665d7.ded47c1b3054e7b2d78c0b86297f36a170b7d2e7980d8c29003634eb58d973d9/bpecodes ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading codes from /home/user/vamvas/.cache/torch/hub/en24k.fastbpe.code ...\n",
      "Read 24000 codes from the codes file.\n",
      "Loading codes from /home/user/vamvas/.cache/torch/hub/ru24k.fastbpe.code ...\n",
      "Read 24000 codes from the codes file.\n",
      "100%|████████████████████████████████████████████████████████████| 1091/1091 [03:56<00:00,  4.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run contrastive conditioning\n",
    "evaluator_model = load_sota_evaluator(\"ru\")\n",
    "\n",
    "mucow_enru.samples = uncovered_samples + covered_samples\n",
    "mucow_enru.categories = {sample.category for sample in mucow_enru.samples}\n",
    "mucow_enru.category_wise_weighting = True\n",
    "mucow_enru.evaluator_model = evaluator_model\n",
    "\n",
    "translations = DictTranslationModel({sample.src_sentence: sample.translation for sample in mucow_enru.samples})\n",
    "contrastive_conditioning_weighted_evaluated_samples = mucow_enru.evaluate(translations).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ccb8760",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create unweighted contrastive conditioning samples\n",
    "contrastive_conditioning_unweighted_evaluated_samples = deepcopy(contrastive_conditioning_weighted_evaluated_samples)\n",
    "for sample in contrastive_conditioning_unweighted_evaluated_samples:\n",
    "    sample.weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34cfa991",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Wrong      0.394     0.429     0.411     126.0\n",
      "     Correct      0.925     0.914     0.919     965.0\n",
      "\n",
      "    accuracy                          0.858    1091.0\n",
      "   macro avg      0.659     0.671     0.665    1091.0\n",
      "weighted avg      0.863     0.858     0.860    1091.0\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Wrong      0.496     0.437     0.465   26743.0\n",
      "     Correct      0.957     0.966     0.962  348295.0\n",
      "\n",
      "    accuracy                          0.928  375038.0\n",
      "   macro avg      0.727     0.702     0.713  375038.0\n",
      "weighted avg      0.924     0.928     0.926  375038.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate contrastive conditioning\n",
    "for evaluated_samples in [\n",
    "    contrastive_conditioning_unweighted_evaluated_samples,\n",
    "    contrastive_conditioning_weighted_evaluated_samples,\n",
    "]:\n",
    "    predicted_labels = []\n",
    "    gold_labels = []\n",
    "    weights = []\n",
    "    for sample in evaluated_samples:\n",
    "        gold_labels.append(int(sample._gold_label))\n",
    "        predicted_labels.append(int(sample.is_correct))\n",
    "        weights.append(getattr(sample, \"weight\", 1))\n",
    "    class_labels = [0, 1]\n",
    "    target_names = [\"Wrong\", \"Correct\"]\n",
    "    print(classification_report(\n",
    "        y_true=gold_labels,\n",
    "        y_pred=predicted_labels,\n",
    "        labels=class_labels,\n",
    "        target_names=target_names,\n",
    "        sample_weight=weights,\n",
    "        zero_division=True,\n",
    "        digits=3,\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
